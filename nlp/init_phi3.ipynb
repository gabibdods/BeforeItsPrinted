{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b1837a",
   "metadata": {
    "title": "Setup & Imports"
   },
   "outputs": [],
   "source": [
    "import os, math, random, json, re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Compute capability:\", torch.cuda.get_device_capability())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837baa34",
   "metadata": {
    "title": "Config"
   },
   "outputs": [],
   "source": [
    "\n",
    "BASE_MODEL = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "DATA_PATH = \"data/train.json\"\n",
    "OUT_DIR = \"out-phi3-lora\"\n",
    "SEED = 7\n",
    "\n",
    "random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "bf16_ok = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if bf16_ok else torch.float16,\n",
    ")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "lora = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "        \"gate_proj\",\"up_proj\",\"down_proj\"\n",
    "    ],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(base, lora)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "ds = load_dataset(\"json\", data_files=DATA_PATH, split=\"train\")\n",
    "print(\"Loaded records:\", len(ds))\n",
    "\n",
    "def build_source(prompt: str) -> str:\n",
    "    return f\"controls: {prompt}\\nReturn ONLY a JSON object.\"\n",
    "\n",
    "MAX_SEQ_LEN = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa855722",
   "metadata": {
    "title": "Preprocessing & Collator"
   },
   "outputs": [],
   "source": [
    "def preprocess_batch(batch: Dict[str, List[Any]]) -> Dict[str, Any]:\n",
    "    input_ids_batch, attn_batch, labels_batch = [], [], []\n",
    "    prompts = batch[\"prompt\"]\n",
    "    outputs = batch[\"output\"]\n",
    "    for p, y in zip(prompts, outputs):\n",
    "        src = build_source(p)\n",
    "        src_ids = tok(src, add_special_tokens=False)[\"input_ids\"]\n",
    "        tgt_ids = tok(y, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "        combined = src_ids + tgt_ids\n",
    "        if len(combined) > MAX_SEQ_LEN:\n",
    "            overflow = len(combined) - MAX_SEQ_LEN\n",
    "            tgt_ids = tgt_ids[overflow:]\n",
    "            combined = src_ids + tgt_ids\n",
    "\n",
    "        labels = [-100] * len(src_ids) + tgt_ids\n",
    "        attn = [1] * len(combined)\n",
    "\n",
    "        input_ids_batch.append(combined)\n",
    "        attn_batch.append(attn)\n",
    "        labels_batch.append(labels)\n",
    "\n",
    "    return {\"input_ids\": input_ids_batch, \"attention_mask\": attn_batch, \"labels\": labels_batch}\n",
    "\n",
    "cols = ds.column_names\n",
    "ds_proc = ds.map(preprocess_batch, batched=True, remove_columns=cols)\n",
    "print(ds_proc)\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForCausal:\n",
    "    tokenizer: Any\n",
    "    pad_to_multiple_of: int = 8\n",
    "    label_pad_token_id: int = -100\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        max_len = max(len(f[\"input_ids\"]) for f in features)\n",
    "        if self.pad_to_multiple_of:\n",
    "            pad_multiple = self.pad_to_multiple_of\n",
    "            if max_len % pad_multiple != 0:\n",
    "                max_len = ((max_len // pad_multiple) + 1) * pad_multiple\n",
    "\n",
    "        input_ids, attn_mask, labels = [], [], []\n",
    "        for f in features:\n",
    "            ids = f[\"input_ids\"]\n",
    "            attn = f[\"attention_mask\"]\n",
    "            lab = f[\"labels\"]\n",
    "\n",
    "            pad_len = max_len - len(ids)\n",
    "            ids = ids + [self.tokenizer.pad_token_id] * pad_len\n",
    "            attn = attn + [0] * pad_len\n",
    "            lab = lab + [self.label_pad_token_id] * pad_len\n",
    "\n",
    "            input_ids.append(ids)\n",
    "            attn_mask.append(attn)\n",
    "            labels.append(lab)\n",
    "\n",
    "        batch = {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attn_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "collator = DataCollatorForCausal(tokenizer=tok, pad_to_multiple_of=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b66e205",
   "metadata": {
    "title": "Train"
   },
   "outputs": [],
   "source": [
    "bf16_ok = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8\n",
    "fp16_ok = torch.cuda.is_available() and not bf16_ok\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.06,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    bf16=bf16_ok,\n",
    "    fp16=fp16_ok,\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_proc,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(OUT_DIR)\n",
    "tok.save_pretrained(OUT_DIR)\n",
    "print(\"Saved LoRA adapter to:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74ca64c",
   "metadata": {
    "title": "Merge (optional)"
   },
   "outputs": [],
   "source": [
    "MERGED_DIR = OUT_DIR + \"-merged\"\n",
    "\n",
    "try:\n",
    "    base_fp16 = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "    peft_loaded = PeftModel.from_pretrained(base_fp16, OUT_DIR)\n",
    "    merged = peft_loaded.merge_and_unload()\n",
    "    os.makedirs(MERGED_DIR, exist_ok=True)\n",
    "    merged.save_pretrained(MERGED_DIR)\n",
    "    tok.save_pretrained(MERGED_DIR)\n",
    "    print(\"Merged model saved to:\", MERGED_DIR)\n",
    "except Exception as e:\n",
    "    print(\"Merge skipped (likely due to VRAM). You can always load the adapter for inference. Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335860d6",
   "metadata": {
    "title": "Inference & Diagnostics"
   },
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# Maybe merge adapters\n",
    "USE_MERGED = False\n",
    "CKPT_DIR = OUT_DIR + (\"-merged\" if USE_MERGED else \"\")\n",
    "\n",
    "if USE_MERGED:\n",
    "    tok_inf = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
    "    if tok_inf.pad_token is None:\n",
    "        tok_inf.pad_token = tok_inf.eos_token\n",
    "    model_inf = AutoModelForCausalLM.from_pretrained(CKPT_DIR, torch_dtype=torch.float16, device_map=\"auto\").eval()\n",
    "else:\n",
    "    tok_inf = AutoTokenizer.from_pretrained(OUT_DIR, use_fast=True)  # tokenizer saved with adapter\n",
    "    if tok_inf.pad_token is None:\n",
    "        tok_inf.pad_token = tok_inf.eos_token\n",
    "    base_q = AutoModelForCausalLM.from_pretrained(BASE_MODEL, quantization_config=bnb_config, device_map=\"auto\")\n",
    "    model_inf = PeftModel.from_pretrained(base_q, OUT_DIR).eval()\n",
    "\n",
    "MAX_SRC_LEN = 512\n",
    "MAX_NEW_TOKENS = 900\n",
    "\n",
    "def make_controls(topics, difficulty, length, fmt):\n",
    "    return f\"topics={','.join(topics)}; difficulty={difficulty}; length={length}; format={'+'.join(fmt)}\"\n",
    "\n",
    "def make_src(ctrl):\n",
    "    return (\n",
    "        f\"controls: {ctrl}\\n\"\n",
    "        \"Return ONLY a JSON object with keys: \"\n",
    "        \"metadata(topics[],difficulty,length,format[]), \"\n",
    "        \"questions[{id:int,text:str,type:multiple_choice|open_answer,options:[str]|null,answer:str,subquestions:null}].\"\n",
    "    )\n",
    "\n",
    "def _balanced_braces(text: str) -> bool:\n",
    "    depth, in_str, esc = 0, False, False\n",
    "    for ch in text:\n",
    "        if in_str:\n",
    "            if esc: esc = False\n",
    "            elif ch == '\\\\': esc = True\n",
    "            elif ch == '\"': in_str = False\n",
    "        else:\n",
    "            if ch == '\"': in_str = True\n",
    "            elif ch == '{': depth += 1\n",
    "            elif ch == '}':\n",
    "                depth -= 1\n",
    "                if depth < 0: return False\n",
    "    return depth == 0 and \"{\" in text\n",
    "\n",
    "class BalancedJSONStop(StoppingCriteria):\n",
    "    def __init__(self, tok): self.tok = tok\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        text = self.tok.decode(input_ids[0], skip_special_tokens=True)\n",
    "        return text.strip().startswith(\"{\") and _balanced_braces(text)\n",
    "\n",
    "brace_ids = tok_inf.encode(\"{\", add_special_tokens=False)\n",
    "first_brace = brace_ids[0] if brace_ids else None\n",
    "assert first_brace is not None, \"Tokenizer couldn't encode '{'.\"\n",
    "def prefix_allowed_tokens_fn(batch_id, input_ids):\n",
    "    return [first_brace] if input_ids.shape[1] == 1 else None\n",
    "\n",
    "def _extract_or_repair(s: str):\n",
    "    s = s.strip()\n",
    "    if s.startswith(\"{\") and s.endswith(\"}\"):\n",
    "        try: return json.loads(s)\n",
    "        except json.JSONDecodeError: pass\n",
    "    m = re.search(r\"\\{[\\s\\S]*\\}\", s)\n",
    "    if m:\n",
    "        cand = m.group(0)\n",
    "        try: return json.loads(cand)\n",
    "        except json.JSONDecodeError:\n",
    "            cand2 = re.sub(r\",(\\s*[}\\]])\", r\"\\1\", cand)\n",
    "            cand2 = cand2.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n",
    "            cand2 = re.sub(r\"\\bNone\\b\", \"null\", cand2)\n",
    "            cand2 = re.sub(r\"\\bTrue\\b\", \"true\", cand2)\n",
    "            cand2 = re.sub(r\"\\bFalse\\b\", \"false\", cand2)\n",
    "            try: return json.loads(cand2)\n",
    "            except Exception: return None\n",
    "    return None\n",
    "\n",
    "def canonicalize(obj):\n",
    "    md = obj.get(\"metadata\", {})\n",
    "    if \"length\" in md:\n",
    "        md[\"length\"] = str(md[\"length\"])\n",
    "    for q in obj.get(\"questions\", []):\n",
    "        if q.get(\"type\") == \"open_answer\":\n",
    "            q[\"options\"] = None\n",
    "        if \"answer\" in q and q[\"answer\"] is not None:\n",
    "            q[\"answer\"] = str(q[\"answer\"])\n",
    "        if \"subquestions\" not in q or q[\"subquestions\"] is None:\n",
    "            q[\"subquestions\"] = None\n",
    "    return obj\n",
    "\n",
    "def generate_exam(topics, difficulty, length, fmt, constrained=True):\n",
    "    ctrl = make_controls(topics, difficulty, length, fmt)\n",
    "    src = make_src(ctrl)\n",
    "    enc = tok_inf(src, return_tensors=\"pt\", truncation=True, max_length=MAX_SRC_LEN).to(model_inf.device)\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=False,\n",
    "        num_beams=5,\n",
    "        length_penalty=0.9,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "    )\n",
    "    if constrained:\n",
    "        gen_kwargs[\"prefix_allowed_tokens_fn\"] = prefix_allowed_tokens_fn\n",
    "        gen_kwargs[\"stopping_criteria\"] = StoppingCriteriaList([BalancedJSONStop(tok_inf)])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model_inf.generate(**enc, **gen_kwargs)\n",
    "    text = tok_inf.decode(out[0], skip_special_tokens=True).strip()\n",
    "    obj  = _extract_or_repair(text)\n",
    "    if isinstance(obj, dict):\n",
    "        return canonicalize(obj), text\n",
    "    return None, text\n",
    "\n",
    "obj, raw = generate_exam([\"algebra\",\"linear-equations\"], \"hard\", 8, [\"multiple_choice\",\"open_answer\"], constrained=True)\n",
    "print(\"RAW (first 500):\\n\", raw[:500])\n",
    "print(\"\\nPARSED:\\n\", json.dumps(obj, indent=2, ensure_ascii=False) if obj else \"FAILED TO PARSE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
