{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b64d0237-1db4-4704-bae3-d95384bb4d6c",
   "metadata": {},
   "source": [
    "# Phi-3 Mini (3.8B) — QLoRA on RTX 4070 (bf16) for JSON Exam Generation\n",
    "\n",
    "This notebook trains a **Phi-3 Mini Instruct** LoRA adapter with QLoRA (int4) on your dataset,\n",
    "adds a robust evaluation metric (**JSON parse rate**), uses a cosine LR schedule + early stopping,\n",
    "and provides **strict** and **creative** decoding that still returns valid JSON.\n",
    "\n",
    "> GPU: **RTX 4070** (bf16-capable). Settings tuned for 12 GB VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87450898",
   "metadata": {
    "title": "Setup & Imports"
   },
   "outputs": [],
   "source": [
    "import os, math, random, json, re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,\n",
    "    Trainer, TrainingArguments, EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Compute capability:\", torch.cuda.get_device_capability())\n",
    "    print(\"BF16 support:\", torch.cuda.get_device_capability()[0] >= 8)\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "try:\n",
    "    from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "    sdpa_kernel([SDPBackend.FLASH_ATTENTION, SDPBackend.MATH, SDPBackend.EFFICIENT_ATTENTION])\n",
    "    print(\"SDPA flash kernels: enabled\")\n",
    "except Exception as e:\n",
    "    print(\"SDPA flash kernels: not enabled\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403c92d8-4c48-4f07-9166-39601b8d9b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, transformers, bitsandbytes as bnb\n",
    "print(\"torch:\", torch.__version__, \"| CUDA:\", torch.version.cuda, \"| cuda_available:\", torch.cuda.is_available())\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"bitsandbytes:\", bnb.__version__)\n",
    "\n",
    "try:\n",
    "    import bitsandbytes.cuda_setup.main as bnb_setup\n",
    "    bnb_setup.main_check()\n",
    "except Exception as e:\n",
    "    print(\"bnb cuda_setup check raised:\", e)\n",
    "\n",
    "import platform, sys\n",
    "print(\"OS:\", platform.platform(), \"| Python:\", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e19945",
   "metadata": {
    "title": "Config & Model"
   },
   "outputs": [],
   "source": [
    "import bitsandbytes\n",
    "\n",
    "BASE_MODEL = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "DATA_PATH = \"data/train.json\"\n",
    "OUT_DIR = \"out-phi3-lora-4070\"\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "bf16_ok = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if bf16_ok else torch.float16,\n",
    ")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"right\"\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "lora = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(base, lora)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "print(\"Loaded model + LoRA for QLoRA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88118918",
   "metadata": {
    "title": "Preprocessing"
   },
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"json\", data_files=DATA_PATH, split=\"train\")\n",
    "print(\"Total records:\", len(ds))\n",
    "\n",
    "def build_source(prompt: str) -> str:\n",
    "    return f\"controls: {prompt}\\nReturn ONLY a JSON object.\"\n",
    "\n",
    "MAX_SEQ_LEN = 1024\n",
    "\n",
    "def preprocess_batch(batch):\n",
    "    input_ids_batch, attn_batch, labels_batch = [], [], []\n",
    "    for p, y in zip(batch[\"prompt\"], batch[\"output\"]):\n",
    "        src = build_source(p)\n",
    "        src_ids = tok(src, add_special_tokens=False)[\"input_ids\"]\n",
    "        tgt_ids = tok(y, add_special_tokens=False)[\"input_ids\"]\n",
    "        combined = src_ids + tgt_ids\n",
    "        if len(combined) > MAX_SEQ_LEN:\n",
    "            overflow = len(combined) - MAX_SEQ_LEN\n",
    "            tgt_ids = tgt_ids[overflow:]\n",
    "            combined = src_ids + tgt_ids\n",
    "        labels = [-100]*len(src_ids) + tgt_ids\n",
    "        attn = [1]*len(combined)\n",
    "        input_ids_batch.append(combined); attn_batch.append(attn); labels_batch.append(labels)\n",
    "    return {\"input_ids\": input_ids_batch, \"attention_mask\": attn_batch, \"labels\": labels_batch}\n",
    "\n",
    "cols = ds.column_names\n",
    "ds_proc = ds.map(preprocess_batch, batched=True, remove_columns=cols)\n",
    "print(ds_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ea8f91",
   "metadata": {
    "title": "Split & Collator"
   },
   "outputs": [],
   "source": [
    "n = len(ds_proc)\n",
    "eval_size = max(1, int(0.1 * n))\n",
    "ds_proc = ds_proc.shuffle(seed=SEED)\n",
    "ds_train = ds_proc.select(range(n - eval_size))\n",
    "ds_eval  = ds_proc.select(range(n - eval_size, n))\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForCausal:\n",
    "    tokenizer: Any\n",
    "    pad_to_multiple_of: int = 8\n",
    "    label_pad_token_id: int = -100\n",
    "    def __call__(self, features: List[Dict[str, Any]]):\n",
    "        max_len = max(len(f[\"input_ids\"]) for f in features)\n",
    "        if self.pad_to_multiple_of:\n",
    "            m = self.pad_to_multiple_of\n",
    "            if max_len % m != 0:\n",
    "                max_len = ((max_len // m) + 1) * m\n",
    "        input_ids, attn_mask, labels = [], [], []\n",
    "        for f in features:\n",
    "            ids, attn, lab = f[\"input_ids\"], f[\"attention_mask\"], f[\"labels\"]\n",
    "            pad_len = max_len - len(ids)\n",
    "            input_ids.append(ids + [tok.pad_token_id]*pad_len)\n",
    "            attn_mask.append(attn + [0]*pad_len)\n",
    "            labels.append(lab + [self.label_pad_token_id]*pad_len)\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attn_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "collator = DataCollatorForCausal(tokenizer=tok, pad_to_multiple_of=8)\n",
    "print(\"Train/Eval sizes:\", len(ds_train), len(ds_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff82eaad",
   "metadata": {
    "title": "Metrics"
   },
   "outputs": [],
   "source": [
    "def _extract_or_repair_text(s: str):\n",
    "    s = s.strip()\n",
    "    if s.startswith(\"{\") and s.endswith(\"}\"):\n",
    "        try: return json.loads(s)\n",
    "        except json.JSONDecodeError: pass\n",
    "    m = re.search(r\"\\{[\\s\\S]*\\}\", s)\n",
    "    if m:\n",
    "        cand = m.group(0)\n",
    "        try: return json.loads(cand)\n",
    "        except json.JSONDecodeError:\n",
    "            cand2 = re.sub(r\",(\\s*[}\\]])\", r\"\\1\", cand)\n",
    "            cand2 = cand2.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n",
    "            cand2 = re.sub(r\"\\bNone\\b\", \"null\", cand2)\n",
    "            cand2 = re.sub(r\"\\bTrue\\b\", \"true\", cand2)\n",
    "            cand2 = re.sub(r\"\\bFalse\\b\", \"false\", cand2)\n",
    "            try: return json.loads(cand2)\n",
    "            except Exception: return None\n",
    "    return None\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds = eval_preds.predictions\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    texts = tok.batch_decode(preds, skip_special_tokens=True)\n",
    "    ok = sum(1 for t in texts if isinstance(_extract_or_repair_text(t), dict))\n",
    "    return {\"json_parse_rate\": ok / max(1, len(texts))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467fd9aa",
   "metadata": {
    "title": "Train with Cosine + EarlyStopping + JSON metric"
   },
   "outputs": [],
   "source": [
    "bf16_ok = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"json_parse_rate\",\n",
    "    greater_is_better=True,\n",
    "    bf16=bf16_ok,\n",
    "    fp16=not bf16_ok,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    "    group_by_length=True,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=900,\n",
    "    generation_num_beams=4,\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=0.5,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_eval,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "trainer.train()\n",
    "model.save_pretrained(OUT_DIR)\n",
    "tok.save_pretrained(OUT_DIR)\n",
    "print(\"Saved LoRA adapter to:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22154c32",
   "metadata": {
    "title": "Merge (optional)"
   },
   "outputs": [],
   "source": [
    "MERGED_DIR = OUT_DIR + \"-merged\"\n",
    "try:\n",
    "    base_fp16 = AutoModelForCausalLM.from_pretrained(BASE_MODEL, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "    peft_loaded = PeftModel.from_pretrained(base_fp16, OUT_DIR)\n",
    "    merged = peft_loaded.merge_and_unload()\n",
    "    os.makedirs(MERGED_DIR, exist_ok=True)\n",
    "    merged.save_pretrained(MERGED_DIR)\n",
    "    tok.save_pretrained(MERGED_DIR)\n",
    "    print(\"Merged model saved to:\", MERGED_DIR)\n",
    "except Exception as e:\n",
    "    print(\"Merge skipped due to environment/VRAM:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bc1779",
   "metadata": {
    "title": "Inference: strict & creative"
   },
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList, BitsAndBytesConfig\n",
    "\n",
    "USE_MERGED = False\n",
    "CKPT_DIR = OUT_DIR + (\"-merged\" if USE_MERGED else \"\")\n",
    "\n",
    "if USE_MERGED:\n",
    "    tok_inf = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=True)\n",
    "    if tok_inf.pad_token is None: tok_inf.pad_token = tok_inf.eos_token\n",
    "    model_inf = AutoModelForCausalLM.from_pretrained(CKPT_DIR, torch_dtype=torch.float16, device_map=\"auto\").eval()\n",
    "else:\n",
    "    tok_inf = AutoTokenizer.from_pretrained(OUT_DIR, use_fast=True)\n",
    "    if tok_inf.pad_token is None: tok_inf.pad_token = tok_inf.eos_token\n",
    "    bf16_ok = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8\n",
    "    base_q = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16 if bf16_ok else torch.float16\n",
    "        ),\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model_inf = PeftModel.from_pretrained(base_q, OUT_DIR).eval()\n",
    "\n",
    "MAX_SRC_LEN = 512\n",
    "MAX_NEW_TOKENS = 900\n",
    "\n",
    "def make_controls(topics, difficulty, length, fmt):\n",
    "    return f\"topics={','.join(topics)}; difficulty={difficulty}; length={length}; format={'+'.join(fmt)}\"\n",
    "\n",
    "def make_src(ctrl):\n",
    "    return (\n",
    "        f\"controls: {ctrl}\\n\"\n",
    "        \"Return ONLY a JSON object with keys: \"\n",
    "        \"metadata(topics[],difficulty,length,format[]), \"\n",
    "        \"questions[{id:int,text:str,type:multiple_choice|open_answer,options:[str]|null,answer:str,subquestions:null}].\"\n",
    "    )\n",
    "\n",
    "def _balanced_braces(text: str) -> bool:\n",
    "    depth, in_str, esc = 0, False, False\n",
    "    for ch in text:\n",
    "        if in_str:\n",
    "            if esc: esc = False\n",
    "            elif ch == '\\\\': esc = True\n",
    "            elif ch == '\"': in_str = False\n",
    "        else:\n",
    "            if ch == '\"': in_str = True\n",
    "            elif ch == '{': depth += 1\n",
    "            elif ch == '}':\n",
    "                depth -= 1\n",
    "                if depth < 0: return False\n",
    "    return depth == 0 and \"{\" in text\n",
    "\n",
    "class BalancedJSONStop(StoppingCriteria):\n",
    "    def __init__(self, tok): self.tok = tok\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        text = self.tok.decode(input_ids[0], skip_special_tokens=True)\n",
    "        return text.strip().startswith(\"{\") and _balanced_braces(text)\n",
    "\n",
    "brace_ids = tok_inf.encode(\"{\", add_special_tokens=False)\n",
    "first_brace = brace_ids[0] if brace_ids else None\n",
    "assert first_brace is not None, \"Tokenizer couldn't encode '{'.\"\n",
    "def prefix_allowed_tokens_fn(batch_id, input_ids):\n",
    "    return [first_brace] if input_ids.shape[1] == 1 else None\n",
    "\n",
    "def _extract_or_repair(s: str):\n",
    "    s = s.strip()\n",
    "    if s.startswith(\"{\") and s.endswith(\"}\"):\n",
    "        try: return json.loads(s)\n",
    "        except json.JSONDecodeError: pass\n",
    "    m = re.search(r\"\\{[\\s\\S]*\\}\", s)\n",
    "    if m:\n",
    "        cand = m.group(0)\n",
    "        try: return json.loads(cand)\n",
    "        except json.JSONDecodeError:\n",
    "            cand2 = re.sub(r\",(\\s*[}\\]])\", r\"\\1\", cand)\n",
    "            cand2 = cand2.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n",
    "            cand2 = re.sub(r\"\\bNone\\b\", \"null\", cand2)\n",
    "            cand2 = re.sub(r\"\\bTrue\\b\", \"true\", cand2)\n",
    "            cand2 = re.sub(r\"\\bFalse\\b\", \"false\", cand2)\n",
    "            try: return json.loads(cand2)\n",
    "            except Exception: return None\n",
    "    return None\n",
    "\n",
    "def canonicalize(obj):\n",
    "    md = obj.get(\"metadata\", {})\n",
    "    if \"length\" in md: md[\"length\"] = str(md[\"length\"])\n",
    "    for q in obj.get(\"questions\", []):\n",
    "        if q.get(\"type\") == \"open_answer\": q[\"options\"] = None\n",
    "        if \"answer\" in q and q[\"answer\"] is not None: q[\"answer\"] = str(q[\"answer\"])\n",
    "        if \"subquestions\" not in q or q[\"subquestions\"] is None: q[\"subquestions\"] = None\n",
    "    return obj\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_exam_strict(topics, difficulty, length, fmt):\n",
    "    ctrl = make_controls(topics, difficulty, length, fmt)\n",
    "    src  = make_src(ctrl)\n",
    "    enc  = tok_inf(src, return_tensors=\"pt\", truncation=True, max_length=MAX_SRC_LEN).to(model_inf.device)\n",
    "    out  = model_inf.generate(\n",
    "        **enc,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=False,\n",
    "        num_beams=5,\n",
    "        length_penalty=0.9,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    "        stopping_criteria=StoppingCriteriaList([BalancedJSONStop(tok_inf)]),\n",
    "    )\n",
    "    text = tok_inf.decode(out[0], skip_special_tokens=True).strip()\n",
    "    obj = _extract_or_repair(text)\n",
    "    return (canonicalize(obj), text) if isinstance(obj, dict) else (None, text)\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_exam_creative(topics, difficulty, length, fmt, temperature=1.1, top_p=0.93, top_k=80, repetition_penalty=1.07):\n",
    "    ctrl = make_controls(topics, difficulty, length, fmt)\n",
    "    src = make_src(ctrl)\n",
    "    enc = tok_inf(src, return_tensors=\"pt\", truncation=True, max_length=MAX_SRC_LEN).to(model_inf.device)\n",
    "    out = model_inf.generate(\n",
    "        **enc,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        no_repeat_ngram_size=3,\n",
    "        prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    "        stopping_criteria=StoppingCriteriaList([BalancedJSONStop(tok_inf)]),\n",
    "    )\n",
    "    text = tok_inf.decode(out[0], skip_special_tokens=True).strip()\n",
    "    obj  = _extract_or_repair(text)\n",
    "    return (canonicalize(obj), text) if isinstance(obj, dict) else (None, text)\n",
    "\n",
    "obj_s, raw_s = generate_exam_strict([\"algebra\",\"linear-equations\"], \"hard\", 12, [\"multiple_choice\",\"open_answer\"])\n",
    "print(\"STRICT RAW (first 500):\\n\", raw_s[:500], \"\\nSTRICT PARSED:\", \"OK\" if obj_s else \"FAIL\")\n",
    "\n",
    "obj_c, raw_c = generate_exam_creative([\"algebra\",\"polynomials\"], \"medium\", 12, [\"multiple_choice\",\"open_answer\"], temperature=1.1)\n",
    "print(\"\\nCREATIVE RAW (first 500):\\n\", raw_c[:500], \"\\nCREATIVE PARSED:\", \"OK\" if obj_c else \"FAIL\")\n",
    "if obj_c:\n",
    "    print(\"\\nCREATIVE JSON keys:\", list(obj_c.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bcdade-a3fc-495f-8466-0eaa37a5c872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
