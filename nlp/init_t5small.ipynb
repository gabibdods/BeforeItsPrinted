{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe70d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, platform, sys, subprocess\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"OS:\", platform.platform())\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA?\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA capability:\", torch.cuda.get_device_capability(0))\n",
    "\n",
    "try:\n",
    "    out = subprocess.check_output([\"nvidia-smi\"], text=True)\n",
    "    print(\"\\n=== nvidia-smi ===\\n\", out)\n",
    "except Exception as e:\n",
    "    print(\"nvidia-smi not available:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5a62ef6aedd828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pathlib, random, uuid, re, collections\n",
    "\n",
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "RAW = DATA_DIR/\"raw\"\n",
    "OUT = DATA_DIR/\"train.json\"\n",
    "RAW.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BEGIN = r'<!--\\s*TOPIC-LIST:BEGIN\\s*-->'\n",
    "END = r'<!--\\s*TOPIC-LIST:END\\s*-->'\n",
    "\n",
    "def controls_to_prompt(ctrl):\n",
    "    length = ctrl.get(\"length\")\n",
    "    length = str(length) if length is not None else \"\"\n",
    "\n",
    "    fmt = ctrl.get(\"format\") or []\n",
    "    if isinstance(fmt, str):\n",
    "        fmt = [fmt]\n",
    "    \n",
    "    topics = ctrl.get(\"topics\") or []\n",
    "    if isinstance(topics, str):\n",
    "        topics = [topics]\n",
    "\n",
    "    parts = [\n",
    "        f\"length={length}\",\n",
    "        f\"format={'+'.join(fmt)}\",\n",
    "        f\"topics={','.join(topics)}\",\n",
    "        f\"difficulty={ctrl.get('difficulty','')}\",\n",
    "    ]\n",
    "    return \"; \".join(parts)\n",
    "\n",
    "def load_api_like_examples():\n",
    "    exams = []\n",
    "    for p in RAW.glob(\"*.json\"):\n",
    "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "            obj = json.load(f)\n",
    "            exams.append(obj)\n",
    "    return exams\n",
    "\n",
    "def normalize_exam(data, *, indent=2) -> str:\n",
    "    def _none_to_empty(o):\n",
    "        if o is None:\n",
    "            return \"\"\n",
    "        if isinstance(o, dict):\n",
    "            return {k: _none_to_empty(v) for k, v in o.items()}\n",
    "        if isinstance(o, list):\n",
    "            return [_none_to_empty(v) for v in o]\n",
    "        return o\n",
    "\n",
    "    cleaned = _none_to_empty(data)\n",
    "    return json.dumps(cleaned, ensure_ascii=False, indent=indent)\n",
    "\n",
    "def load_topics_from_readme(path=\"README.md\", region_required=True, return_codes=False, dedupe=True, sort=False):\n",
    "    text = pathlib.Path(path).read_text(encoding=\"utf-8\")\n",
    "\n",
    "    m = re.search(f'{BEGIN}(.*?){END}', text, flags=re.S | re.I)\n",
    "    region = m.group(1) if m else (text if not region_required else \"\")\n",
    "\n",
    "    cat_re = re.compile(r'^\\s*###\\s+\\*\\*\\s*\\d+\\.\\s*(?P<cat>[^*]+?)\\s*\\*\\*\\s*$', re.M)\n",
    "    topic_re = re.compile(r'^\\s*\\*\\s+(?P<topic>.+?)(?:\\s*-\\s*(?P<codes>(?:\\d{4}|)(?:,(?:\\d{4}|)){0,2}))?\\s*$', re.M)\n",
    "\n",
    "    by_cat = collections.defaultdict(list)\n",
    "    current = None\n",
    "\n",
    "    for line in region.splitlines():\n",
    "        line = line.rstrip()\n",
    "\n",
    "        mcat = cat_re.match(line)\n",
    "        if mcat:\n",
    "            current = mcat.group('cat').strip()\n",
    "            continue\n",
    "\n",
    "        mtop = topic_re.match(line)\n",
    "        if mtop and current:\n",
    "            t = mtop.group('topic').strip()\n",
    "            t = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', t)\n",
    "            t = re.sub(r'\\*(.*?)\\*', r'\\1', t)\n",
    "            t = re.sub(r'\\s+', ' ', t).strip()\n",
    "\n",
    "            if return_codes:\n",
    "                codes_raw = mtop.group('codes')\n",
    "                codes = [c or None for c in (codes_raw.split(',') if codes_raw is not None else [])]\n",
    "                by_cat[current].append((t, codes))\n",
    "            else:\n",
    "                by_cat[current].append(t)\n",
    "\n",
    "    if return_codes:\n",
    "        flat = [(t, codes) for topics in by_cat.values() for (t, codes) in topics]\n",
    "    else:\n",
    "        flat = [t for topics in by_cat.values() for t in topics]\n",
    "\n",
    "    if dedupe:\n",
    "        if return_codes:\n",
    "            seen, uniq = set(), []\n",
    "            for t, c in flat:\n",
    "                if t.lower() in seen:\n",
    "                    continue\n",
    "                seen.add(t.lower()); uniq.append((t, c))\n",
    "            flat = uniq\n",
    "        else:\n",
    "            flat = list(dict.fromkeys([t for t in flat]))  # order-preserving\n",
    "    if sort:\n",
    "        if return_codes:\n",
    "            flat.sort(key=lambda x: x[0].casefold())\n",
    "        else:\n",
    "            flat.sort(key=str.casefold)\n",
    "\n",
    "    return flat, dict(by_cat)\n",
    "\n",
    "def debug_readme(path=\"README.md\"):\n",
    "    text = pathlib.Path(path).read_text(encoding=\"utf-8\")\n",
    "    print(\"Has BEGIN marker:\", bool(re.search(BEGIN, text, re.I)))\n",
    "    print(\"Has END marker:  \", bool(re.search(END, text, re.I)))\n",
    "    print(\"Has category hdr:\", bool(re.search(r'^\\s*###\\s+\\*\\*\\s*\\d+\\.', text, re.M)))\n",
    "    print(\"Has bullets:     \", bool(re.search(r'^\\s*\\*\\s+', text, re.M)))\n",
    "\n",
    "topics, by_cat = load_topics_from_readme(\"README.md\")\n",
    "print(len(topics), \"topics loaded\")\n",
    "print(topics[:10])\n",
    "\n",
    "random.seed(7)\n",
    "exams = load_api_like_examples()\n",
    "\n",
    "difficulties = [\"easy\", \"medium\", \"hard\"]\n",
    "format_sets = [\n",
    "    [\"multiple_choice\"],\n",
    "    [\"open_answer\"],\n",
    "    [\"multiple_choice\", \"open_answer\"],\n",
    "]\n",
    "\n",
    "records = []\n",
    "for ex in exams:\n",
    "    ctrl = {\n",
    "        \"topics\": random.sample(topics, k=1),  # ensure list\n",
    "        \"difficulty\": random.choice(difficulties),\n",
    "        \"length\": random.choice([20, 15, 25]),\n",
    "        \"format\": random.choice(format_sets),   # ensure list\n",
    "    }\n",
    "    prompt = controls_to_prompt(ctrl)\n",
    "    input_text = \"Exam format:\\n\" + normalize_exam(ex)\n",
    "    target = normalize_exam(ex)\n",
    "\n",
    "    records.append({\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"prompt\": prompt,\n",
    "        \"input\": input_text,\n",
    "        \"output\": target\n",
    "    })\n",
    "\n",
    "OUT.write_text(\"\\n\".join(json.dumps(r, ensure_ascii=False) for r in records), encoding=\"utf-8\")\n",
    "len(records), str(OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9207912d914be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast, DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"t5-small\"\n",
    "DATA_PATH = \"data/train.json\"\n",
    "OUT_DIR = \"out-t5-lora\"\n",
    "\n",
    "def format_example(ex):\n",
    "    src = f\"controls: {ex['prompt']}\\n\\nexemplars:\\n{ex['input']}\\n\\n# task: generate new exam as JSON\"\n",
    "    tgt = ex[\"output\"]\n",
    "    return {\"src\": src, \"tgt\": tgt}\n",
    "\n",
    "tok = T5TokenizerFast.from_pretrained(MODEL_NAME)\n",
    "base = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "lora = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    target_modules=[\"q\", \"k\", \"v\", \"o\"],\n",
    ")\n",
    "model = get_peft_model(base, lora)\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.enable_input_require_grads()\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "assert any(p.requires_grad for _, p in model.named_parameters() if \"lora\" in _), \\\n",
    "    \"No LoRA parameters marked trainable. Check `target_modules` names.\"\n",
    "\n",
    "ds = load_dataset(\"json\", data_files=DATA_PATH, split=\"train\")\n",
    "ds = ds.map(format_example)\n",
    "\n",
    "max_src_len = 512\n",
    "max_tgt_len = 512\n",
    "\n",
    "def tok_map(batch):\n",
    "    model_inputs = tok(batch[\"src\"], max_length=max_src_len, truncation=True)\n",
    "    labels = tok(text_target=batch[\"tgt\"], max_length=max_tgt_len, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "ds = ds.map(tok_map, batched=True, remove_columns=ds.column_names)\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tok, model=model, pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "bf16_ok = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8\n",
    "fp16_ok = torch.cuda.is_available() and not bf16_ok\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    bf16=bf16_ok,\n",
    "    fp16=fp16_ok,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"none\",\n",
    "    group_by_length=True,\n",
    "    remove_unused_columns=False,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(OUT_DIR)\n",
    "tok.save_pretrained(OUT_DIR)\n",
    "print(\"Dumped\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cb571b-e71a-4341-8992-7fd757165515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "from peft import PeftModel, PeftConfig\n",
    "import os\n",
    "\n",
    "CKPT = \"out-t5-lora\"\n",
    "\n",
    "print(\"Folder contains:\", os.listdir(CKPT))\n",
    "\n",
    "try:\n",
    "    peft_cfg = PeftConfig.from_pretrained(CKPT)\n",
    "    print(\"PEFT adapter base_model_name_or_path:\", peft_cfg.base_model_name_or_path)\n",
    "except Exception as e:\n",
    "    print(\"Not a PEFT adapter folder or unreadable:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69672102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, random, torch\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast, StoppingCriteria, StoppingCriteriaList\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE = \"t5-small\"\n",
    "CKPT = \"out-t5-lora\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "MAX_SRC_LEN = 512\n",
    "MAX_NEW_TOKENS = 700\n",
    "SEED = 7\n",
    "torch.manual_seed(SEED); random.seed(SEED)\n",
    "\n",
    "tok = T5TokenizerFast.from_pretrained(CKPT)\n",
    "base = T5ForConditionalGeneration.from_pretrained(BASE)\n",
    "model = PeftModel.from_pretrained(base, CKPT).eval().to(DEVICE)\n",
    "\n",
    "# Maybe merge adapters\n",
    "#from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "#from peft import PeftModel\n",
    "#BASE, CKPT, MERGED = \"t5-small\", \"out-t5-lora\", \"out-t5-lora-merged\"\n",
    "#tok = T5TokenizerFast.from_pretrained(CKPT)\n",
    "#base = T5ForConditionalGeneration.from_pretrained(BASE)\n",
    "#merged = PeftModel.from_pretrained(base, CKPT).merge_and_unload()\n",
    "#merged.save_pretrained(MERGED); tok.save_pretrained(MERGED)\n",
    "#print(\"Saved merged model to\", MERGED)\n",
    "\n",
    "\n",
    "def make_controls(topics, difficulty, length, fmt):\n",
    "    return f\"topics={','.join(topics)}; difficulty={difficulty}; length={length}; format={'+'.join(fmt)}\"\n",
    "\n",
    "def make_src(ctrl):\n",
    "    instr = (\n",
    "        \"Return ONLY a JSON object with keys: \"\n",
    "        \"metadata(topics[],difficulty,length,format[]), \"\n",
    "        \"questions[{id:int,text:str,type:multiple_choice|open_answer,options:[str]|null,answer:str,subquestions:null}].\"\n",
    "    )\n",
    "    return f\"controls: {ctrl}\\n{instr}\"\n",
    "\n",
    "def token_len(s): return len(tok(s).input_ids)\n",
    "\n",
    "def _balanced_braces(text: str) -> bool:\n",
    "    depth, in_str, esc = 0, False, False\n",
    "    for ch in text:\n",
    "        if in_str:\n",
    "            if esc: esc = False\n",
    "            elif ch == '\\\\': esc = True\n",
    "            elif ch == '\"': in_str = False\n",
    "        else:\n",
    "            if ch == '\"': in_str = True\n",
    "            elif ch == '{': depth += 1\n",
    "            elif ch == '}':\n",
    "                depth -= 1\n",
    "                if depth < 0: return False\n",
    "    return depth == 0 and \"{\" in text\n",
    "\n",
    "class BalancedJSONStop(StoppingCriteria):\n",
    "    def __init__(self, tokenizer): self.tok = tokenizer\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        text = self.tok.decode(input_ids[0], skip_special_tokens=True)\n",
    "        return text.strip().startswith(\"{\") and _balanced_braces(text)\n",
    "\n",
    "def _extract_or_repair(s: str):\n",
    "    s = s.strip()\n",
    "    if s.startswith(\"{\") and s.endswith(\"}\"):\n",
    "        try: return json.loads(s)\n",
    "        except json.JSONDecodeError: pass\n",
    "    m = re.search(r\"\\{[\\s\\S]*\\}\", s)\n",
    "    if m:\n",
    "        cand = m.group(0)\n",
    "        try: return json.loads(cand)\n",
    "        except json.JSONDecodeError:\n",
    "            cand2 = re.sub(r\",(\\s*[}\\]])\", r\"\\1\", cand)  # trailing commas\n",
    "            cand2 = cand2.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n",
    "            cand2 = re.sub(r\"\\bNone\\b\", \"null\", cand2)\n",
    "            cand2 = re.sub(r\"\\bTrue\\b\", \"true\", cand2)\n",
    "            cand2 = re.sub(r\"\\bFalse\\b\", \"false\", cand2)\n",
    "            try: return json.loads(cand2)\n",
    "            except Exception: return None\n",
    "    return None\n",
    "\n",
    "def canonicalize(obj):\n",
    "    md = obj.get(\"metadata\", {})\n",
    "    if \"length\" in md: md[\"length\"] = str(md[\"length\"])\n",
    "    for q in obj.get(\"questions\", []):\n",
    "        if q.get(\"type\") == \"open_answer\": q[\"options\"] = None\n",
    "        if \"answer\" in q and q[\"answer\"] is not None: q[\"answer\"] = str(q[\"answer\"])\n",
    "        if \"subquestions\" not in q or q[\"subquestions\"] is None: q[\"subquestions\"] = None\n",
    "    return obj\n",
    "\n",
    "def generate_once(ctrl, constrained=False):\n",
    "    src = make_src(ctrl)\n",
    "    enc = tok(src, return_tensors=\"pt\", max_length=MAX_SRC_LEN, truncation=True).to(DEVICE)\n",
    "    print(f\"Tokens: src={token_len(src)} (truncated to {enc['input_ids'].shape[-1]}), MAX_SRC_LEN={MAX_SRC_LEN}\")\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=MAX_NEW_TOKENS, do_sample=False, num_beams=5,\n",
    "        length_penalty=0.9, early_stopping=True, no_repeat_ngram_size=3,\n",
    "    )\n",
    "    if constrained:\n",
    "        dec_start = tok(\"{\", return_tensors=\"pt\").input_ids.to(DEVICE)\n",
    "        gen_kwargs[\"decoder_input_ids\"] = dec_start\n",
    "        gen_kwargs[\"stopping_criteria\"] = StoppingCriteriaList([BalancedJSONStop(tok)])\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(**enc, **gen_kwargs)\n",
    "    raw = tok.decode(out_ids[0], skip_special_tokens=True).strip()\n",
    "    obj = _extract_or_repair(raw)\n",
    "    status = \"OK\" if isinstance(obj, dict) else \"PARSE_FAIL\"\n",
    "    print(f\"[{'CONSTRAINED' if constrained else 'RAW'}] status={status}, chars={len(raw)}\")\n",
    "    if len(raw) > 800:\n",
    "        print(raw[:800] + \"\\n...[truncated]...\")\n",
    "    else:\n",
    "        print(raw)\n",
    "    if isinstance(obj, dict):\n",
    "        obj = canonicalize(obj)\n",
    "    return obj, raw\n",
    "\n",
    "ctrl = make_controls([\"algebra\",\"linear-equations\"], \"hard\", 8, [\"multiple_choice\",\"open_answer\"])\n",
    "\n",
    "print(\"=== RAW GENERATION ===\")\n",
    "obj_raw, raw_text = generate_once(ctrl, constrained=False)\n",
    "\n",
    "print(\"\\n=== CONSTRAINED GENERATION ===\")\n",
    "obj_con, con_text = generate_once(ctrl, constrained=True)\n",
    "\n",
    "final = obj_con if isinstance(obj_con, dict) else obj_raw\n",
    "\n",
    "if isinstance(final, dict):\n",
    "    print(\"\\nParsed JSON (canonicalized):\")\n",
    "    print(json.dumps(final, indent=2, ensure_ascii=False))\n",
    "else:\n",
    "    print(\"\\nNo valid JSON parsed. Inspect raw above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7651d7ad-6f3b-49fe-bd80-ab75d19b05ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
